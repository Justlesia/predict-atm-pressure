{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Atmosfer pressure. Construction of a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Description of data\n",
    "\n",
    "The system of automatic state ambient air monitoring stations in Lithuania consists of 14 urban air quality monitoring stations operating in Vilnius, Kaunas, Klaipėda, Šiauliai, Panevėžys, Jonava, Kėdainiai, Naujoji Akmenė and Mažeikiai and 3 integrated monitoring stations operating in Aukštaitija, Žemaitija and Dzūkija National Parks.\n",
    "\n",
    "Concentrations of the following pollutants are measured at automatic air quality monitoring stations: particulate matter PM10, fine particulate matter PM2.5, nitrogen oxides (NO2, NOx, NO), sulfur dioxide (SO2), carbon monoxide (CO), ozone (O3), benzene, mercury.\n",
    "The tests and measurements shall be carried out in accordance with the requirements of Directives 2004/107/EC of the European Parliament and of the Council relating to arsenic, cadmium, mercury, nickel and polycyclic aromatic hydrocarbons in ambient air and 2008/50/EC on ambient air quality and cleaner air for Europe.\n",
    "\n",
    "\n",
    "The data consists of files obtained from different sources:\n",
    "\n",
    "* Averages.csv\n",
    "* Quantity.csv\n",
    "* QuantityUnits.csv\n",
    "* Station.csv\n",
    "\n",
    "https://data.gov.lt/datasets/500/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ## Loading the library self-written functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lesia/anaconda3/lib/python3.11/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "#from catboost import CatBoostRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "%config InlineBackend.figure_format='retina'\n",
    "#arnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "#warnings.simplefilter(action=\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from numpy import asarray\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bold(): \n",
    "    return \"\\033[1m\"\n",
    "\n",
    "def bold_end(): \n",
    "    return \"\\033[0m\"\n",
    "\n",
    "#Ставим формат для нумериков\n",
    "pd.options.display.float_format = '{: >10.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#**Function print_basic_info, to display information about the array and its variables.**\n",
    "\n",
    "#* base - database name\n",
    "#* info - 1: output information about the array, other: no output\n",
    "#* describe - 1: output description of array variables, other: no output\n",
    "#* duplicate - 1: display the number of complete duplicates\n",
    "#* head - n: output example base (output n - lines), n < 1: no output\n",
    "\n",
    "def print_basic_info(base, info, describe, duplicat, head):\n",
    "    if info == 1:\n",
    "        print(\"\\n\", bold(), colored('info','green'), bold_end(), \"\\n\")\n",
    "        print( base.info())  \n",
    "    if head >= 1:\n",
    "        print(\"\\n\", bold(),colored('head','green'),bold_end())\n",
    "        display(base.head(head))\n",
    "    if describe == 1:\n",
    "        print(\"\\n\", bold(),colored('describe','green'),bold_end(),\"\\n\")\n",
    "        for i in base.columns:\n",
    "            print(\"\\n\", bold(), colored(i,'blue'),bold_end(),\"\\n\", base[i].describe())\n",
    "    if duplicat == 1:\n",
    "        print(\"\\n\", bold(),colored('duplicated','green'),bold_end(),\"\\n\")\n",
    "        print(base[base.duplicated() == True][base.columns[0]].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_averages = pd.read_csv('datasets/Averages.csv', sep=',',decimal='.')\n",
    "data_quantity = pd.read_csv('datasets/Quantity.csv', sep=',',decimal='.')\n",
    "data_quantity_units = pd.read_csv('datasets/QuantityUnits.csv', sep=',',decimal='.')\n",
    "data_station = pd.read_csv('datasets/Station.csv', sep=',',decimal='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- # data_arc = pd.read_csv('/datasets/final_steel/data_arc.csv')\n",
    "# data_bulk = pd.read_csv('/datasets/final_steel/data_bulk.csv')\n",
    "# data_bulk_time = pd.read_csv('/datasets/final_steel/data_bulk_time.csv')\n",
    "# data_gas = pd.read_csv('/datasets/final_steel/data_gas.csv')\n",
    "# data_temp = pd.read_csv('/datasets/final_steel/data_temp.csv')\n",
    "# data_wire = pd.read_csv('/datasets/final_steel/data_wire.csv')\n",
    "# data_wire_time = pd.read_csv('/datasets/final_steel/data_wire_time.csv') -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_basic_info(data_averages,1,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data\n",
    "data_averages['datetime'] = pd.to_datetime(data_averages.ldatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_averages['datetime'].hist(bins = 50, alpha=0.5, density=True)\n",
    "\n",
    "plt.xlabel('dates')\n",
    "plt.ylabel('Distribution density')\n",
    "plt.title('datetime distribution by sample', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2627777 cases of monitoring.\n",
    "\n",
    "Distribution by time is shows the data is more in 2020 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_basic_info(data_quantity,1,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quantity.longname.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "90 entries. it is a dictionary what measurements are present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_basic_info(data_quantity_units,1,1,1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "962 entries. it is a dictionary about Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_basic_info(data_station,1,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_station['latitude'].hist(bins = 50, alpha=0.5, density=True)\n",
    "data_station['longitude'].hist(bins = 50, alpha=0.5, density=True)\n",
    "\n",
    "plt.title('latitude and longitude', fontsize=15) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22 entries. it is a dictionary about location (latitude and longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Cleaning\n",
    "\n",
    "* Where it is necessary to calculate the information by batch and combine the data into one database.\n",
    "* Leave only the necessary columns, including so that there is no information inaccessible in real cases (leaks).\n",
    "* We throw it out if the time is the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine \n",
    "data = data_averages.merge(data_quantity_units,how='left', on = 'code_combi')\n",
    "\n",
    "# look at target\n",
    "data.groupby(by=\"code_unit._id\", dropna=False)['lvalue'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data measurements\n",
    "\n",
    "data['year'] = data.datetime.dt.year\n",
    "data['month'] = data.datetime.dt.month\n",
    "data['weekday'] = data.datetime.dt.weekday\n",
    "data['day'] = data.datetime.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add data_quantity\n",
    "d_q = data_quantity[data_quantity['code_unit._id'] == '67461e7c-506e-430a-8962-cd25ebed54da'] \n",
    "data = data.merge(d_q,how='left', on = 'code_unit._id')\n",
    "\n",
    "#look for atm.pressure\n",
    "data_quantity['code_unit._id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave atm.pressure and 2023 year\n",
    "data = data[data['year']>= 2023]\n",
    "data =data[data['code_unit._id'] == '67461e7c-506e-430a-8962-cd25ebed54da']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at target\n",
    "data.groupby(by=\"year\", dropna=False)['lvalue'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add data_station\n",
    "\n",
    "data_station['stat_num._id'] = data_station['_id']\n",
    "data = data.merge(data_station,how='left', on = 'stat_num._id',suffixes=('_xx', '_yy'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution Future target\n",
    "print(bold(),\"% mean in test sample: lvalue\",colored(round(data['lvalue'].mean(),2),'blue'))\n",
    "\n",
    "data['lvalue'].hist(bins = 50, alpha=0.5, density=True)\n",
    "\n",
    "plt.xlabel('lvalue')\n",
    "plt.ylabel('Distribution density')\n",
    "plt.title('lvalue distribution by sample', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# look at 0.05\n",
    "data['lvalue'].quantile(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# look at 0.99\n",
    "data['lvalue'].quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# del 0,05\n",
    "data = data[data['lvalue'] > 968]\n",
    "#data = data[data['lvalue'] < 1025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Distribution Future target\n",
    "print(bold(),\"% mean in test sample: lvalue\",colored(round(data['lvalue'].mean(),2),'blue'))\n",
    "\n",
    "data['lvalue'].hist(bins = 50, alpha=0.5, density=True)\n",
    "\n",
    "plt.xlabel('lvalue')\n",
    "plt.ylabel('Distribution density')\n",
    "plt.title('lvalue distribution by sample', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#data = data[['lvalue','stat_num','code_quantity._id','year','month','weekday','day','latitude','longitude']]\n",
    "\n",
    "# leave our data columns\n",
    "data = data[['lvalue','stat_num','year','month','weekday','day','latitude','longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data[['lvalue','stat_num','year','month','weekday','day','latitude','longitude']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['stat_num'] = data.stat_num.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_basic_info(data,1,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# drop_duplicates\n",
    "\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preparing data for the model\n",
    "\n",
    "* Dividing the date set into test and training/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#select target\n",
    "features, target = data.drop(['lvalue'], axis=1), data['lvalue']\n",
    "features = features.fillna(0)\n",
    "\n",
    "#we divide into target and test (25%)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, \n",
    "                                                                            test_size=0.10, shuffle=False\n",
    "                                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(asarray(features_train[['latitude','longitude']]))\n",
    "\n",
    "features_train[['latitude','longitude']] = scaler.transform(asarray(features_train[['latitude','longitude']]))\n",
    "features_test[['latitude','longitude']] = scaler.transform(asarray(features_test[['latitude','longitude']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = features_train\n",
    "X_test = features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building a model\n",
    "    \n",
    "* Identify a pool of approaches that takes into account possible limitations and find a regression model that satisfies the necessary characteristics. \n",
    "* Check the model for adequacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#divide into folders\n",
    "cv = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#RandomForestClassifier\n",
    "\n",
    "grid = {'max_depth' : [i for i in np.arange(12,20)]}\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "\n",
    "gs = GridSearchCV(clf, grid, cv=cv, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "# temporarily comment out to reduce the calculation time\n",
    "gs.fit(X_train,target_train)\n",
    "\n",
    "RandomForestRegressor_params = gs.best_params_\n",
    "print(RandomForestRegressor_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#XGBRegressor\n",
    "\n",
    "grid = {'eta' : [i for i in np.arange(0.1,0.3,0.1)] , 'max_depth' : [i for i in np.arange(14,20)]}\n",
    "\n",
    "clf = XGBRegressor(random_state = 123)\n",
    "\n",
    "gs = GridSearchCV(clf, grid, cv=cv, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "# temporarily comment out to reduce the calculation time\n",
    "#gs.fit(X_train,target_train)\n",
    "\n",
    "#XGBRegressor_best_params = gs.best_params_\n",
    "#print(XGBRegressor_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#GradientBoostingRegressor\n",
    "\n",
    "grid = { 'loss' : ['huber'],\n",
    "        'learning_rate' : [i for i in np.arange(0.1,0.4,0.1)] , \n",
    "        #'max_depth' : [i for i in np.arange(12,17,1)]\n",
    "       }\n",
    "\n",
    "clf = GradientBoostingRegressor(random_state = 123)\n",
    "\n",
    "gs = GridSearchCV(clf, grid, cv=cv, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "# temporarily comment out to reduce the calculation time\n",
    "##gs.fit(X_train,target_train)\n",
    "\n",
    "#GradientBoostingRegressor_best_params = gs.best_params_\n",
    "#print(GradientBoostingRegressor_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#LGBMRegressor\n",
    "\n",
    "grid = { 'learning_rate' : [i for i in np.arange(0.1,0.3,0.1)]}\n",
    "\n",
    "clf = LGBMRegressor(random_state = 123)\n",
    "\n",
    "gs = GridSearchCV(clf, grid, cv=cv, scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "# temporarily comment out to reduce the calculation time\n",
    "#gs.fit(X_train,target_train)\n",
    "\n",
    "#LGBMRegressor_best_params = gs.best_params_\n",
    "#print(LGBMRegressor_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##let's make a function that will record the training time, prediction speed, and prediction quality\n",
    "def put_in_base(model_name, base_res, features_train, target_train):\n",
    "    features_train.reset_index(drop = True, inplace = True)\n",
    "    target_train.reset_index(drop = True, inplace = True)\n",
    "    cv = TimeSeriesSplit(n_splits=3)\n",
    "    time_train, time_predict, MAE = [], [], []\n",
    "    for train_index, val_index in cv.split(features_train):\n",
    "        X_train, X_val = features_train.loc[train_index], features_train.loc[val_index]\n",
    "        y_train, y_val = target_train.loc[train_index], target_train.loc[val_index]\n",
    "        #training time.\n",
    "        start_time = time.time()\n",
    "        model_name.fit(X_train, y_train)\n",
    "        time_train.append(round((time.time() - start_time),3))\n",
    "        #prediction speed.\n",
    "        start_time = time.time()\n",
    "        predictions_valid = model_name.predict(X_val)\n",
    "        time_predict.append(round((time.time() - start_time),3))\n",
    "        #quality of pre-order(MAY)\n",
    "        MAE.append(mean_absolute_error(y_val, predictions_valid))\n",
    "    \n",
    "    if len((str(clf).split('(')[0]).split('.')) == 1:\n",
    "        base_res.loc[str(clf).split('(')[0],'time_train'] = np.mean(time_train)\n",
    "        base_res.loc[str(clf).split('(')[0],'time_predict'] = np.mean(time_predict)\n",
    "        base_res.loc[str(clf).split('(')[0],'MAE'] = np.mean(MAE)\n",
    "    else:\n",
    "        nm = ((str(clf).split('(')[0]).split('.')[2]).split(' ')[0]\n",
    "        base_res.loc[nm,'time_train'] = np.mean(time_train)\n",
    "        base_res.loc[nm,'time_predict'] = np.mean(time_predict)\n",
    "        base_res.loc[nm,'MAE'] = np.mean(MAE)\n",
    "        \n",
    "    return base_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##let's run it for our models with previously found parameters\n",
    "ans = pd.DataFrame()\n",
    "\n",
    "for clf in (LinearRegression(),RandomForestRegressor(max_depth = 19),\n",
    "            XGBRegressor(eta = 0.2, max_depth = 16, random_state = 123),\n",
    "            GradientBoostingRegressor(learning_rate = 0.4, loss = 'huber', max_depth = 13, random_state = 123),\n",
    "            LGBMRegressor(learning_rate = 0.2, random_state = 123),\n",
    "           ):\n",
    "    put_in_base(clf, ans, X_train,target_train)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(ans.sort_values(by = 'MAE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = GradientBoostingRegressor(learning_rate = 0.4, loss = 'huber', max_depth = 13, random_state = 123)\n",
    "\n",
    "final_model.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importances = pd.DataFrame()\n",
    "df_importances['feature'] = data.drop(['lvalue'], axis=1).columns\n",
    "df_importances['importance'] = final_model.feature_importances_\n",
    "df_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del year and stat_num and longitude\n",
    "\n",
    "X_train = X_train[['month','weekday','day','latitude', 'longitude']]\n",
    "X_test = X_test[['month','weekday','day','latitude','longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def put_in_base_test(model_name, base_res, features_train, target_train, features_test, target_test):\n",
    "    \n",
    "    #training time.\n",
    "    start_time = time.time()\n",
    "    model_name.fit(features_train, target_train)\n",
    "    \n",
    "    if len((str(model_name).split('(')[0]).split('.')) == 1:\n",
    "        nm = str(model_name).split('(')[0]\n",
    "    else:\n",
    "        nm = ((str(model_name).split('(')[0]).split('.')[2]).split(' ')[0]\n",
    "\n",
    "    base_res.loc[nm,'time_train'] = round((time.time() - start_time),3)\n",
    "    #prediction speed.\n",
    "    start_time = time.time()\n",
    "    predictions_test = model_name.predict(features_test)\n",
    "    base_res.loc[nm,'time_predict'] = round((time.time() - start_time),3)\n",
    "    #prediction quality(mae)\n",
    "    base_res.loc[nm,'MAE'] = mean_absolute_error(target_test, predictions_test)\n",
    "    \n",
    "    return predictions_test, model_name, base_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# let's look at the result\n",
    "info_final_test = pd.DataFrame()\n",
    "\n",
    "predictions_test, final_model, info_final_test = put_in_base_test(final_model, info_final_test, \n",
    "                                                                  X_train, target_train, \n",
    "                                                                  X_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "colors = ['rgb(107, 174, 214)']\n",
    "fig = go.Figure(data=[go.Table( header=dict(\n",
    "    values=['Время обучения модели (в секундах)', \n",
    "            'Время предсказания модели (в секундах)',\n",
    "            'MAE на тестовой выборке'],\n",
    "    line_color='white', fill_color='white',\n",
    "    align='center', font=dict(color='black', size=15)\n",
    "  ),\n",
    "  cells=dict(\n",
    "    values=[info_final_test.time_train.round(2), \n",
    "            info_final_test.time_predict.round(2),info_final_test.MAE.round(2)],\n",
    "    line_color=[colors], fill_color=[colors],\n",
    "    align='center', font=dict(color='black', size=14)\n",
    "  ))\n",
    "])\n",
    "fig.update_layout(title_text=\"Показатели финальной модели (GradientBoostingRegressor)\", height = 300)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Checking the model for adequacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(bold(),\"% mean in test sample: target\",colored(round(target_test.mean(),2),'blue')+bold(),\"VS predictions\",\n",
    "      colored(round(predictions_test.mean() ,2),'blue'),bold_end())\n",
    "\n",
    "target_test.hist(bins = 50, alpha=0.5, density=True)\n",
    "pd.Series(predictions_test).hist(bins = 50, alpha=0.5, density=True)\n",
    "\n",
    "plt.xlabel('Температура нагрева')\n",
    "plt.ylabel('Плотность распределения')\n",
    "plt.title('Распределение на тестовой выборке', fontsize=15) \n",
    "plt.legend(['Таргет', 'Предсказания'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The averages are similar. Distributions also, however, the prediction is expected to determine the extreme cases worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Compare dummy\n",
    "dummy_model = DummyRegressor(strategy='median')\n",
    "\n",
    "predictions_dummy, dummy_model, info_final_test = put_in_base_test(dummy_model, info_final_test, \n",
    "                                                                  features_train, target_train, \n",
    "                                                                  features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(info_final_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our model shows itself a little better than Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Output.\n",
    "<a name=\"15.\"></a>\n",
    "[<font size=\"2\">(to the content)</font>](#1common.)\n",
    "\n",
    "* The resulting model has MAY 6.67 on the test sample\n",
    "\n",
    "Based on the results of the initial analysis and the work taken:\n",
    "# atm.pressure and 2023 year\n",
    "\n",
    "***Target attribute - volume (value)***\n",
    "\n",
    "**The model is built using XGBRegressor(eta = 0.2, max_depth = 16, random_state = 123):**\n",
    "\n",
    "This model has poor performance comparasing to DummyRegressor and should not be used in the prod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "171px",
    "width": "242px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}